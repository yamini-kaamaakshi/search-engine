Index: src/lib/simple-search.ts
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import { getUploadedDocuments, splitDocumentIntoChunks } from './file-processor';\nimport { generateOllamaEmbedding } from './ollama-embeddings';\nimport { searchSimilarDocuments } from './qdrant';\nimport type { StoredDocument } from './qdrant';\n\ninterface SearchableDocument {\n  id: string;\n  title: string;\n  content: string;\n  type: 'qa' | 'file';\n  source?: string;\n  filename?: string;\n  chunkIndex?: number;\n}\n\nasync function getAllSearchableDocuments(): Promise<SearchableDocument[]> {\n  const documents: SearchableDocument[] = [];\n\n  // Only use uploaded files (no pre-loaded Q&A data)\n  const uploadedFiles = await getUploadedDocuments();\n  uploadedFiles.forEach(file => {\n    const chunks = splitDocumentIntoChunks(file, 500);\n    chunks.forEach(chunk => {\n      documents.push({\n        id: chunk.id,\n        title: chunk.filename,\n        content: chunk.content,\n        type: 'file',\n        source: chunk.filename,\n        filename: chunk.filename,\n        chunkIndex: chunk.chunkIndex\n      });\n    });\n  });\n\n  return documents;\n}\n\n// Simple text similarity function\nfunction calculateSimilarity(query: string, text: string): number {\n  const queryWords = query.toLowerCase().split(/\\s+/);\n  const textWords = text.toLowerCase().split(/\\s+/);\n\n  let matches = 0;\n  queryWords.forEach(queryWord => {\n    if (textWords.some(textWord => textWord.includes(queryWord) || queryWord.includes(textWord))) {\n      matches++;\n    }\n  });\n\n  return matches / queryWords.length;\n}\n\nexport async function searchDocuments(query: string, limit: number = 5) {\n  try {\n    // First try to use vector search with Qdrant\n    const queryEmbedding = await generateOllamaEmbedding(query);\n    const searchResults = await searchSimilarDocuments(queryEmbedding, limit);\n\n    // Convert Qdrant results to our expected format\n    const results = searchResults.map(result => {\n      const payload = result.payload as any;\n\n      return {\n        id: String(result.id),\n        title: payload.filename || 'Untitled',\n        content: payload.content || '',\n        type: payload.type || 'file',\n        source: payload.filename || '',\n        filename: payload.filename || '',\n        chunkIndex: payload.chunkIndex,\n        score: result.score || 0,\n      };\n    });\n\n    // If we have results from Qdrant, return them\n    if (results.length > 0) {\n      return results;\n    }\n\n    // Otherwise fall back to simple text search\n    return simpleTextSearch(query, limit);\n\n  } catch (error) {\n    console.error('Error in vector search, falling back to text search:', error);\n    // Fallback to simple text search\n    return await simpleTextSearch(query, limit);\n  }\n}\n\n// Simple text-based search as fallback\nasync function simpleTextSearch(query: string, limit: number = 5) {\n  const allDocuments = await getAllSearchableDocuments();\n\n  // If no documents are uploaded, return empty results\n  if (allDocuments.length === 0) {\n    return [];\n  }\n\n  // Calculate similarity scores for each document\n  const results = allDocuments.map(doc => {\n    const score = calculateSimilarity(query, doc.content);\n\n    return {\n      id: doc.id,\n      title: doc.title,\n      content: doc.content,\n      type: doc.type,\n      source: doc.source,\n      filename: doc.filename,\n      chunkIndex: doc.chunkIndex,\n      score,\n    };\n  });\n\n  // Sort by score and return top results\n  return results\n    .filter(result => result.score > 0)\n    .sort((a, b) => b.score - a.score)\n    .slice(0, limit);\n}\n\n// Ollama client for generating responses\nexport async function callOllama(prompt: string): Promise<string> {\n  try {\n    const response = await fetch(`${process.env.OLLAMA_HOST || 'http://localhost:11434'}/api/generate`, {\n      method: 'POST',\n      headers: {\n        'Content-Type': 'application/json',\n      },\n      body: JSON.stringify({\n        model: process.env.OLLAMA_MODEL || 'llama3.2:3b',\n        prompt: prompt,\n        stream: false,\n      }),\n    });\n\n    if (!response.ok) {\n      throw new Error(`Ollama API error: ${response.statusText}`);\n    }\n\n    const data = await response.json();\n    return data.response;\n  } catch (error) {\n    console.error('Error calling Ollama:', error);\n    throw error;\n  }\n}\n\nexport async function generateAnswer(query: string, relevantDocs: any[]) {\n  // If no documents found, return a helpful message\n  if (!relevantDocs || relevantDocs.length === 0) {\n    return \"I couldn't find any relevant information in the uploaded documents. Please make sure you have uploaded documents related to your question.\";\n  }\n\n  const context = relevantDocs.map(doc => {\n    return `Source: ${doc.filename || doc.source}\\nContent: ${doc.content}`;\n  }).join('\\n\\n');\n\n  const prompt = `You are a search assistant helping to find relevant CVs/resumes. Based on the provided CVs below, create a helpful summary of the candidates found.\n\nContext - Relevant CVs:\n${context}\n\nUser Query: ${query}\n\nInstructions:\n1. Summarize the key qualifications and experience from each CV\n2. Mention the candidate names and their roles\n3. Highlight relevant skills and technologies\n4. Keep the response concise and professional\n5. Always cite the source filename\n\nProvide a summary of the candidates found:`;\n\n  return await callOllama(prompt);\n}
===================================================================
diff --git a/src/lib/simple-search.ts b/src/lib/simple-search.ts
--- a/src/lib/simple-search.ts	(revision 8ae01e5e3c7cbdbc4e4c7d70d10839a4bf0c3c4c)
+++ b/src/lib/simple-search.ts	(date 1759646266470)
@@ -1,7 +1,6 @@
 import { getUploadedDocuments, splitDocumentIntoChunks } from './file-processor';
 import { generateOllamaEmbedding } from './ollama-embeddings';
 import { searchSimilarDocuments } from './qdrant';
-import type { StoredDocument } from './qdrant';
 
 interface SearchableDocument {
   id: string;
